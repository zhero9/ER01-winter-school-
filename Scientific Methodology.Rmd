---
title: "R Notebook"
author: "Shad, Edin"
date: "december 1, 2016"
output:
  html_document: default
  html_notebook: default
---

### Scientific methodology and preformance evaluation 
# Winter School, ENS Lyon 


We start with looking at the data provided on the web page: http://mescal.imag.fr/membres/arnaud.legrand/teaching/2013/M2R_EP_archive_quicksort.tgz

We won't focus on the particular implementation, although this is a very important part of the experiment, we leave it for the future work. We rather analyse the data, as we would do if somebody would give us implementation as black boxes which we query. One way to think about this, is that we do not compare in general which type of implementation works the best, but rather which out of three implementation is better for different sizes of arrays. So we turn towards determining when we should use one of these three algorithms to get faster running times. 

```{r}
data = read.csv("archive_quicksort/measurements.csv")
df = data.frame(data)
plot(df)

summary(df)
```

The above plot didn't say much. We observe that Size and Type are discrete variables and that Time should be assumed as continous. In order to see more, lets separate data according to the algorithm used. 

```{r}
library(ggplot2)
ggplot(df, aes(x = Size, y = Time, colour = Type,  group = Type)) + geom_point() + ggtitle("Data corresponding to each algorithm")
```
Obsearvtions: sizes for which the algorithms were tested are quite scarce and we can not conclude much. If we would have just this data, we could try to fit linear regression for each of the algorithms. Even if the results of linear regresion return good values in terms of R-value and p-values, we could be missing some strange behaviours since our data is so scarce. In any way we do the linear regresion for data of each algorithm. Before that, we plot again data of each algorithm separately just to see if they look linear.
```{r, echo = FALSE}
library(dplyr)
library(broom)

```

```{r}
para = df[df$Type == " Parallel",] 
sequ = df[df$Type == " Sequential",]
builtIn = df[df$Type == " Built-in",]

ggplot(para, aes(x = Size, y= Time)) + geom_point(size = 0.7) + ggtitle("Plot for parallel algorithms.")
```

```{r}
ggplot(sequ, aes(x = Size, y= Time)) + geom_point(size = 0.7) + ggtitle("Plot for sequential algorithms.")

```


```{r}
ggplot(builtIn, aes(x = Size, y= Time)) + geom_point(size = 0.7) + ggtitle("Plot for built-in algorithms.")
```
With exception of one outliner for parallel algorithm, we see that data resemble a linear function

```{r}

regp = lm(data = para, Time ~ Size)
summary(regp)
plot(x = para$Size,y = para$Time)
abline(regp)
```
Results of linear fit for parallel algorithm seems quite strong for this data.

```{r}

regq = lm(data = sequ, Time ~ Size)
summary(regq)
par(mfrow~c(1,1))
plot(x = sequ$Size,y = sequ$Time)
abline(regq)


```


```{r}
regb = lm(data = builtIn, Time ~ Size)
summary(regb)
par(mfrow~c(1,1))
plot(x = builtIn$Size,y = builtIn$Time)
abline(regb)
```
```{r}
ggplot(df, aes(x = Size, y = Time, colour = Type,  group = Type)) + geom_point() + ggtitle("Data with regresion lines for each of algorithms") + geom_abline(intercept = coef(regp)["(Intercept)"], slope = coef(regp)["Size"], colour = "green" ) +geom_abline(intercept = coef(regq)["(Intercept)"], slope = coef(regq)["Size"], colour = "blue" ) + geom_abline(intercept = coef(regb)["(Intercept)"], slope = coef(regb)["Size"], colour = "red" )
```
`Under above assumptions, the parallel algorithm seems to outrun the other two algorithms for the arrays of size bigger than 250000 and that sequential and built-in algorithm are alsmost the same. From this data it is not reliable to say that sequential is overall better than built-in since the data set is small, and not well calibrated. One more problem of the current data is that we don't know how were they measured, on how many machines and what kind of machines. In order to improve this, we generate new data sets using providede "black-boxes" and extend the data for sizes up to milion. It is worth noting that the R-squared values are very high. Especially for the sequential and the built-in regression we obtain values of around 99% but for the parallel a value around 89. This could suggest that the parallel implementation is not linear.
```{r}
dataEdin = read.csv("MyData/measurements.csv")
dE = data.frame(dataEdin)
summary(dE)
```
```{r}
ggplot(dE, aes(x = Size, y = Time, colour = Type,  group = Type)) + geom_point() + ggtitle("Plot for different types of algorithms")
```
 We observe that the paralel algorithm shows very slow running time for smaller arrays. But it seems that it will be better after some big enough size. That is way we for the moment stop here, and try to get more data for the biggest sizes of arrays. But first, let us see what happens with the same procedure on different arhitecture. 

```{r}
dataShad = read.csv("ShadMeasurements.csv")
dS = data.frame(dataShad)
summary(dS)
```
```{r}
ggplot(dS, aes(x = Size, y = Time, colour = Type,  group = Type)) + geom_point() + ggtitle("Plot for different types of algorithms(1m)")
ggplot(dE, aes(x = Size, y = Time, colour = Type,  group = Type)) + geom_point() + ggtitle("Plot for different types of algorithms(1m)")

ggplot(dS[dS$Type == " Sequential",],  aes(x = Size, y = Time, colour = Type,  group = Type)) + geom_point(colour="#619CFF") + ggtitle("Sequential algorithm on machine S (1m)")
ggplot(dE[dE$Type == " Sequential",] , aes(x = Size, y = Time, colour = Type,  group = Type)) + geom_point(colour="#619CFF") + ggtitle("Sequential algorithm on machine E (1m")

ggplot(dS[dS$Type == " Parallel",],  aes(x = Size, y = Time, colour = Type,  group = Type)) + geom_point(colour="#00BA38") + ggtitle("Parallel algorithm on machine S (1m")
ggplot(dE[dE$Type == " Parallel",] , aes(x = Size, y = Time, colour = Type,  group = Type)) + geom_point(colour="#00BA38") + ggtitle("Parallel algorithm on machine E (1m")

ggplot(dS[dS$Type == " Built-in",],  aes(x = Size, y = Time, colour = Type,  group = Type)) + geom_point(colour = "#F8766D") + ggtitle("Built-in  algorithm on machine S (1m)")
ggplot(dE[dE$Type == " Built-in",] , aes(x = Size, y = Time, colour = Type,  group = Type)) + geom_point(colour = "#F8766D") + ggtitle("Built-in  algorithm on machine E (1m)")



```

We observe that in both cases the parallel algorithm doesn't follow linear scaling, while both sequential and built-in algorithm show linear increase of execution time with the size. We also decide that for a better estimation we need to increase the maximum size. We move the maximum up to two milion.

???? Guided by known expected running time of quicksort we prose the logarithmic-linear model. To get some more information and intuition about the performances of each implementation of quicksort we extend the measurements for sizes up to two milion.

```{r}
dataEdin2 = read.csv("MyData2milion/measurements.csv")
dE2 = data.frame(dataEdin2)
summary(dE2)
```
We again plot it, to get more intution. 

```{r}
ggplot(dE2, aes(x = Size, y = Time, colour = Type,  group = Type)) + geom_point() + ggtitle("Plot for different types of algorithms (2 milion)")
```
```{r}
dataShad2 = read.csv("ShadMeasurements2.csv")
dS2 = data.frame(dataShad2)
summary(dS2)
```
```{r}
ggplot(dS2, aes(x = Size, y = Time, colour = Type,  group = Type)) + geom_point() + ggtitle("Plot for different types of algorithms (2 milion)")
```
We see again that the Parallel algorithm does not really follow a linear model but we try it noonetheles.
```{r}
paraS2 = dS2[dS2$Type == " Parallel",] 
regpS2 <- lm(data=paraS2, Time~Size)
summary(regpS2)
regpS2log <- lm(data=paraS2, Time~log(Size))
summary(regpS2log)
```
By doing the a linear regression anyway we find that the adjusted R-squared value is around 0.76. We do a logarithmic regression and retrieve an adjusted R-squared value of 0.83 which could suggest that the model is better. However we also note that the std. error is larger for the logarithmic model compared to the linear model.
```{r}
para2 = dS2[dS2$Type == "Parallel",]
test <- function(x) {coef(regpS2log)["log(Size)"]*log(x) + coef(regpS2log)["(Intercept)"]}
ggplot(dS2, aes(x = Size, y = Time, colour = Type,  group = Type)) + geom_point() + ggtitle("Data with regresion lines for each of algorithms") + stat_function(fun=test,color="green")
```

